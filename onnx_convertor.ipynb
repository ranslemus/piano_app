{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97f80b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "W1210 11:18:50.378000 19328 site-packages\\torch\\onnx\\_internal\\exporter\\_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 17 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `PianoModelSmall2D([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `PianoModelSmall2D([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 17).\n",
      "Failed to convert the model to the target version 17 using the ONNX C API. The model was not modified\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\onnxscript\\version_converter\\__init__.py\", line 127, in call\n",
      "    converted_proto = _c_api_utils.call_onnx_api(\n",
      "  File \"c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\onnxscript\\version_converter\\_c_api_utils.py\", line 65, in call_onnx_api\n",
      "    result = func(proto)\n",
      "  File \"c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\onnxscript\\version_converter\\__init__.py\", line 122, in _partial_convert_version\n",
      "    return onnx.version_converter.convert_version(\n",
      "  File \"c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\onnx\\version_converter.py\", line 39, in convert_version\n",
      "    converted_model_str = C.convert_version(model_str, target_version)\n",
      "RuntimeError: D:\\a\\onnx\\onnx\\onnx/version_converter/adapters/axes_input_to_attribute.h:65: adapt: Assertion `node->hasAttribute(kaxes)` failed: No initializer or constant input to node found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 41 of general pattern rewrite rules.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ONNXProgram(\n",
       "    model=\n",
       "        <\n",
       "            ir_version=10,\n",
       "            opset_imports={'': 18},\n",
       "            producer_name='pytorch',\n",
       "            producer_version='2.9.1',\n",
       "            domain=None,\n",
       "            model_version=None,\n",
       "        >\n",
       "        graph(\n",
       "            name=main_graph,\n",
       "            inputs=(\n",
       "                %\"input\"<FLOAT,[s77,1,480,640]>\n",
       "            ),\n",
       "            outputs=(\n",
       "                %\"output\"<FLOAT,[1,88]>\n",
       "            ),\n",
       "            initializers=(\n",
       "                %\"preprocess.0.weight\"<FLOAT,[32,1,3,3]>{Tensor(...)},\n",
       "                %\"preprocess.0.bias\"<FLOAT,[32]>{Tensor(...)},\n",
       "                %\"final_conv.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"fc.bias\"<FLOAT,[88]>{TorchTensor(...)},\n",
       "                %\"blocks.0.main.0.weight\"<FLOAT,[32,32,1,1]>{Tensor(...)},\n",
       "                %\"blocks.0.main.3.weight\"<FLOAT,[32,32,3,3]>{Tensor(...)},\n",
       "                %\"blocks.0.main.7.weight\"<FLOAT,[32,32,1,1]>{Tensor(...)},\n",
       "                %\"blocks.0.downsample.0.weight\"<FLOAT,[32,32,3,3]>{Tensor(...)},\n",
       "                %\"blocks.1.main.0.weight\"<FLOAT,[64,32,1,1]>{Tensor(...)},\n",
       "                %\"blocks.1.main.3.weight\"<FLOAT,[64,64,3,3]>{Tensor(...)},\n",
       "                %\"blocks.1.main.7.weight\"<FLOAT,[64,64,1,1]>{Tensor(...)},\n",
       "                %\"blocks.1.downsample.0.weight\"<FLOAT,[64,32,3,3]>{Tensor(...)},\n",
       "                %\"blocks.2.main.0.weight\"<FLOAT,[128,64,1,1]>{Tensor(...)},\n",
       "                %\"blocks.2.main.3.weight\"<FLOAT,[128,128,3,3]>{Tensor(...)},\n",
       "                %\"blocks.2.main.7.weight\"<FLOAT,[128,128,1,1]>{Tensor(...)},\n",
       "                %\"blocks.2.downsample.0.weight\"<FLOAT,[128,64,3,3]>{Tensor(...)},\n",
       "                %\"blocks.3.main.0.weight\"<FLOAT,[128,128,1,1]>{Tensor(...)},\n",
       "                %\"blocks.3.main.3.weight\"<FLOAT,[128,128,3,3]>{Tensor(...)},\n",
       "                %\"blocks.3.main.7.weight\"<FLOAT,[128,128,1,1]>{Tensor(...)},\n",
       "                %\"blocks.3.downsample.0.weight\"<FLOAT,[128,128,3,3]>{Tensor(...)},\n",
       "                %\"blocks.4.main.0.weight\"<FLOAT,[256,128,1,1]>{Tensor(...)},\n",
       "                %\"blocks.4.main.3.weight\"<FLOAT,[256,256,3,3]>{Tensor(...)},\n",
       "                %\"blocks.4.main.7.weight\"<FLOAT,[256,256,1,1]>{Tensor(...)},\n",
       "                %\"blocks.4.downsample.0.weight\"<FLOAT,[256,128,3,3]>{Tensor(...)},\n",
       "                %\"final_conv.weight\"<FLOAT,[256,256,3]>{TorchTensor(...)},\n",
       "                %\"fc.weight\"<FLOAT,[88,40960]>{TorchTensor(...)},\n",
       "                %\"val_189\"<INT64,[1]>{Tensor<INT64,[1]>(array([2]), name='val_189')},\n",
       "                %\"val_193\"<INT64,[2]>{Tensor<INT64,[2]>(array([    1, 40960]), name='val_193')},\n",
       "                %\"leaky_relu_bias\"<FLOAT,[32]>{Tensor(...)},\n",
       "                %\"leaky_relu_1_bias\"<FLOAT,[32]>{Tensor(...)},\n",
       "                %\"leaky_relu_2_bias\"<FLOAT,[32]>{Tensor(...)},\n",
       "                %\"leaky_relu_3_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"leaky_relu_4_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"leaky_relu_5_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"leaky_relu_6_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"leaky_relu_7_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"leaky_relu_8_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"leaky_relu_9_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"leaky_relu_10_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"leaky_relu_11_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"leaky_relu_12_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"leaky_relu_13_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"leaky_relu_14_bias\"<FLOAT,[256]>{Tensor(...)}\n",
       "            ),\n",
       "        ) {\n",
       "             0 |  # node_Conv_254\n",
       "                  %\"getitem\"<FLOAT,[1,32,480,640]> ⬅️ ::Conv(%\"input\", %\"preprocess.0.weight\"{...}, %\"preprocess.0.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "             1 |  # node_leaky_relu\n",
       "                  %\"leaky_relu\"<FLOAT,[1,32,480,640]> ⬅️ ::LeakyRelu(%\"getitem\") {alpha=0.01}\n",
       "             2 |  # node_Conv_256\n",
       "                  %\"getitem_3\"<FLOAT,[1,32,480,640]> ⬅️ ::Conv(%\"leaky_relu\", %\"blocks.0.main.0.weight\"{...}, %\"leaky_relu_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             3 |  # node_leaky_relu_1\n",
       "                  %\"leaky_relu_1\"<FLOAT,[1,32,480,640]> ⬅️ ::LeakyRelu(%\"getitem_3\") {alpha=0.01}\n",
       "             4 |  # node_Conv_258\n",
       "                  %\"getitem_6\"<FLOAT,[1,32,240,320]> ⬅️ ::Conv(%\"leaky_relu_1\", %\"blocks.0.main.3.weight\"{...}, %\"leaky_relu_1_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(2, 2), pads=(1, 1, 1, 1)}\n",
       "             5 |  # node_leaky_relu_2\n",
       "                  %\"leaky_relu_2\"<FLOAT,[1,32,240,320]> ⬅️ ::LeakyRelu(%\"getitem_6\") {alpha=0.01}\n",
       "             6 |  # node_Conv_260\n",
       "                  %\"getitem_9\"<FLOAT,[1,32,240,320]> ⬅️ ::Conv(%\"leaky_relu_2\", %\"blocks.0.main.7.weight\"{...}, %\"leaky_relu_2_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             7 |  # node_Conv_262\n",
       "                  %\"getitem_12\"<FLOAT,[1,32,240,320]> ⬅️ ::Conv(%\"leaky_relu\", %\"blocks.0.downsample.0.weight\"{...}, %\"leaky_relu_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(2, 2), pads=(1, 1, 1, 1)}\n",
       "             8 |  # node_add_5\n",
       "                  %\"add_5\"<FLOAT,[1,32,240,320]> ⬅️ ::Add(%\"getitem_9\", %\"getitem_12\")\n",
       "             9 |  # node_leaky_relu_3\n",
       "                  %\"leaky_relu_3\"<FLOAT,[1,32,240,320]> ⬅️ ::LeakyRelu(%\"add_5\") {alpha=0.01}\n",
       "            10 |  # node_Conv_264\n",
       "                  %\"getitem_15\"<FLOAT,[1,64,240,320]> ⬅️ ::Conv(%\"leaky_relu_3\", %\"blocks.1.main.0.weight\"{...}, %\"leaky_relu_3_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "            11 |  # node_leaky_relu_4\n",
       "                  %\"leaky_relu_4\"<FLOAT,[1,64,240,320]> ⬅️ ::LeakyRelu(%\"getitem_15\") {alpha=0.01}\n",
       "            12 |  # node_Conv_266\n",
       "                  %\"getitem_18\"<FLOAT,[1,64,120,160]> ⬅️ ::Conv(%\"leaky_relu_4\", %\"blocks.1.main.3.weight\"{...}, %\"leaky_relu_4_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(2, 2), pads=(1, 1, 1, 1)}\n",
       "            13 |  # node_leaky_relu_5\n",
       "                  %\"leaky_relu_5\"<FLOAT,[1,64,120,160]> ⬅️ ::LeakyRelu(%\"getitem_18\") {alpha=0.01}\n",
       "            14 |  # node_Conv_268\n",
       "                  %\"getitem_21\"<FLOAT,[1,64,120,160]> ⬅️ ::Conv(%\"leaky_relu_5\", %\"blocks.1.main.7.weight\"{...}, %\"leaky_relu_5_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "            15 |  # node_Conv_270\n",
       "                  %\"getitem_24\"<FLOAT,[1,64,120,160]> ⬅️ ::Conv(%\"leaky_relu_3\", %\"blocks.1.downsample.0.weight\"{...}, %\"leaky_relu_3_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(2, 2), pads=(1, 1, 1, 1)}\n",
       "            16 |  # node_add_6\n",
       "                  %\"add_6\"<FLOAT,[1,64,120,160]> ⬅️ ::Add(%\"getitem_21\", %\"getitem_24\")\n",
       "            17 |  # node_leaky_relu_6\n",
       "                  %\"leaky_relu_6\"<FLOAT,[1,64,120,160]> ⬅️ ::LeakyRelu(%\"add_6\") {alpha=0.01}\n",
       "            18 |  # node_Conv_272\n",
       "                  %\"getitem_27\"<FLOAT,[1,128,120,160]> ⬅️ ::Conv(%\"leaky_relu_6\", %\"blocks.2.main.0.weight\"{...}, %\"leaky_relu_6_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "            19 |  # node_leaky_relu_7\n",
       "                  %\"leaky_relu_7\"<FLOAT,[1,128,120,160]> ⬅️ ::LeakyRelu(%\"getitem_27\") {alpha=0.01}\n",
       "            20 |  # node_Conv_274\n",
       "                  %\"getitem_30\"<FLOAT,[1,128,60,160]> ⬅️ ::Conv(%\"leaky_relu_7\", %\"blocks.2.main.3.weight\"{...}, %\"leaky_relu_7_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(2, 1), pads=(1, 1, 1, 1)}\n",
       "            21 |  # node_leaky_relu_8\n",
       "                  %\"leaky_relu_8\"<FLOAT,[1,128,60,160]> ⬅️ ::LeakyRelu(%\"getitem_30\") {alpha=0.01}\n",
       "            22 |  # node_Conv_276\n",
       "                  %\"getitem_33\"<FLOAT,[1,128,60,160]> ⬅️ ::Conv(%\"leaky_relu_8\", %\"blocks.2.main.7.weight\"{...}, %\"leaky_relu_8_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "            23 |  # node_Conv_278\n",
       "                  %\"getitem_36\"<FLOAT,[1,128,60,160]> ⬅️ ::Conv(%\"leaky_relu_6\", %\"blocks.2.downsample.0.weight\"{...}, %\"leaky_relu_6_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(2, 1), pads=(1, 1, 1, 1)}\n",
       "            24 |  # node_add_7\n",
       "                  %\"add_7\"<FLOAT,[1,128,60,160]> ⬅️ ::Add(%\"getitem_33\", %\"getitem_36\")\n",
       "            25 |  # node_leaky_relu_9\n",
       "                  %\"leaky_relu_9\"<FLOAT,[1,128,60,160]> ⬅️ ::LeakyRelu(%\"add_7\") {alpha=0.01}\n",
       "            26 |  # node_Conv_280\n",
       "                  %\"getitem_39\"<FLOAT,[1,128,60,160]> ⬅️ ::Conv(%\"leaky_relu_9\", %\"blocks.3.main.0.weight\"{...}, %\"leaky_relu_9_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "            27 |  # node_leaky_relu_10\n",
       "                  %\"leaky_relu_10\"<FLOAT,[1,128,60,160]> ⬅️ ::LeakyRelu(%\"getitem_39\") {alpha=0.01}\n",
       "            28 |  # node_Conv_282\n",
       "                  %\"getitem_42\"<FLOAT,[1,128,30,160]> ⬅️ ::Conv(%\"leaky_relu_10\", %\"blocks.3.main.3.weight\"{...}, %\"leaky_relu_10_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(2, 1), pads=(1, 1, 1, 1)}\n",
       "            29 |  # node_leaky_relu_11\n",
       "                  %\"leaky_relu_11\"<FLOAT,[1,128,30,160]> ⬅️ ::LeakyRelu(%\"getitem_42\") {alpha=0.01}\n",
       "            30 |  # node_Conv_284\n",
       "                  %\"getitem_45\"<FLOAT,[1,128,30,160]> ⬅️ ::Conv(%\"leaky_relu_11\", %\"blocks.3.main.7.weight\"{...}, %\"leaky_relu_11_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "            31 |  # node_Conv_286\n",
       "                  %\"getitem_48\"<FLOAT,[1,128,30,160]> ⬅️ ::Conv(%\"leaky_relu_9\", %\"blocks.3.downsample.0.weight\"{...}, %\"leaky_relu_9_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(2, 1), pads=(1, 1, 1, 1)}\n",
       "            32 |  # node_add_8\n",
       "                  %\"add_8\"<FLOAT,[1,128,30,160]> ⬅️ ::Add(%\"getitem_45\", %\"getitem_48\")\n",
       "            33 |  # node_leaky_relu_12\n",
       "                  %\"leaky_relu_12\"<FLOAT,[1,128,30,160]> ⬅️ ::LeakyRelu(%\"add_8\") {alpha=0.01}\n",
       "            34 |  # node_Conv_288\n",
       "                  %\"getitem_51\"<FLOAT,[1,256,30,160]> ⬅️ ::Conv(%\"leaky_relu_12\", %\"blocks.4.main.0.weight\"{...}, %\"leaky_relu_12_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "            35 |  # node_leaky_relu_13\n",
       "                  %\"leaky_relu_13\"<FLOAT,[1,256,30,160]> ⬅️ ::LeakyRelu(%\"getitem_51\") {alpha=0.01}\n",
       "            36 |  # node_Conv_290\n",
       "                  %\"getitem_54\"<FLOAT,[1,256,15,160]> ⬅️ ::Conv(%\"leaky_relu_13\", %\"blocks.4.main.3.weight\"{...}, %\"leaky_relu_13_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(2, 1), pads=(1, 1, 1, 1)}\n",
       "            37 |  # node_leaky_relu_14\n",
       "                  %\"leaky_relu_14\"<FLOAT,[1,256,15,160]> ⬅️ ::LeakyRelu(%\"getitem_54\") {alpha=0.01}\n",
       "            38 |  # node_Conv_292\n",
       "                  %\"getitem_57\"<FLOAT,[1,256,15,160]> ⬅️ ::Conv(%\"leaky_relu_14\", %\"blocks.4.main.7.weight\"{...}, %\"leaky_relu_14_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "            39 |  # node_Conv_294\n",
       "                  %\"getitem_60\"<FLOAT,[1,256,15,160]> ⬅️ ::Conv(%\"leaky_relu_12\", %\"blocks.4.downsample.0.weight\"{...}, %\"leaky_relu_12_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(2, 1), pads=(1, 1, 1, 1)}\n",
       "            40 |  # node_add_9\n",
       "                  %\"add_9\"<FLOAT,[1,256,15,160]> ⬅️ ::Add(%\"getitem_57\", %\"getitem_60\")\n",
       "            41 |  # node_leaky_relu_15\n",
       "                  %\"leaky_relu_15\"<FLOAT,[1,256,15,160]> ⬅️ ::LeakyRelu(%\"add_9\") {alpha=0.01}\n",
       "            42 |  # node_mean\n",
       "                  %\"mean\"<FLOAT,[1,256,160]> ⬅️ ::ReduceMean(%\"leaky_relu_15\", %\"val_189\"{[2]}) {keepdims=0, noop_with_empty_axes=0}\n",
       "            43 |  # node_conv1d\n",
       "                  %\"conv1d\"<FLOAT,[1,256,160]> ⬅️ ::Conv(%\"mean\", %\"final_conv.weight\"{...}, %\"final_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1,), strides=(1,), pads=(1, 1)}\n",
       "            44 |  # node_view\n",
       "                  %\"view\"<FLOAT,[1,40960]> ⬅️ ::Reshape(%\"conv1d\", %\"val_193\"{[1, 40960]}) {allowzero=1}\n",
       "            45 |  # node_linear\n",
       "                  %\"output\"<FLOAT,[1,88]> ⬅️ ::Gemm(%\"view\", %\"fc.weight\"{...}, %\"fc.bias\"{...}) {transA=0, transB=1, alpha=1.0, beta=1.0}\n",
       "            return %\"output\"<FLOAT,[1,88]>\n",
       "        }\n",
       "\n",
       "\n",
       "    ,\n",
       "    exported_program=\n",
       "        ExportedProgram:\n",
       "            class GraphModule(torch.nn.Module):\n",
       "                def forward(self, p_preprocess_0_weight: \"f32[32, 1, 3, 3]\", p_preprocess_0_bias: \"f32[32]\", p_preprocess_1_weight: \"f32[32]\", p_preprocess_1_bias: \"f32[32]\", p_blocks_0_main_0_weight: \"f32[32, 32, 1, 1]\", p_blocks_0_main_1_weight: \"f32[32]\", p_blocks_0_main_1_bias: \"f32[32]\", p_blocks_0_main_3_weight: \"f32[32, 32, 3, 3]\", p_blocks_0_main_4_weight: \"f32[32]\", p_blocks_0_main_4_bias: \"f32[32]\", p_blocks_0_main_7_weight: \"f32[32, 32, 1, 1]\", p_blocks_0_main_8_weight: \"f32[32]\", p_blocks_0_main_8_bias: \"f32[32]\", p_blocks_0_downsample_0_weight: \"f32[32, 32, 3, 3]\", p_blocks_0_downsample_1_weight: \"f32[32]\", p_blocks_0_downsample_1_bias: \"f32[32]\", p_blocks_1_main_0_weight: \"f32[64, 32, 1, 1]\", p_blocks_1_main_1_weight: \"f32[64]\", p_blocks_1_main_1_bias: \"f32[64]\", p_blocks_1_main_3_weight: \"f32[64, 64, 3, 3]\", p_blocks_1_main_4_weight: \"f32[64]\", p_blocks_1_main_4_bias: \"f32[64]\", p_blocks_1_main_7_weight: \"f32[64, 64, 1, 1]\", p_blocks_1_main_8_weight: \"f32[64]\", p_blocks_1_main_8_bias: \"f32[64]\", p_blocks_1_downsample_0_weight: \"f32[64, 32, 3, 3]\", p_blocks_1_downsample_1_weight: \"f32[64]\", p_blocks_1_downsample_1_bias: \"f32[64]\", p_blocks_2_main_0_weight: \"f32[128, 64, 1, 1]\", p_blocks_2_main_1_weight: \"f32[128]\", p_blocks_2_main_1_bias: \"f32[128]\", p_blocks_2_main_3_weight: \"f32[128, 128, 3, 3]\", p_blocks_2_main_4_weight: \"f32[128]\", p_blocks_2_main_4_bias: \"f32[128]\", p_blocks_2_main_7_weight: \"f32[128, 128, 1, 1]\", p_blocks_2_main_8_weight: \"f32[128]\", p_blocks_2_main_8_bias: \"f32[128]\", p_blocks_2_downsample_0_weight: \"f32[128, 64, 3, 3]\", p_blocks_2_downsample_1_weight: \"f32[128]\", p_blocks_2_downsample_1_bias: \"f32[128]\", p_blocks_3_main_0_weight: \"f32[128, 128, 1, 1]\", p_blocks_3_main_1_weight: \"f32[128]\", p_blocks_3_main_1_bias: \"f32[128]\", p_blocks_3_main_3_weight: \"f32[128, 128, 3, 3]\", p_blocks_3_main_4_weight: \"f32[128]\", p_blocks_3_main_4_bias: \"f32[128]\", p_blocks_3_main_7_weight: \"f32[128, 128, 1, 1]\", p_blocks_3_main_8_weight: \"f32[128]\", p_blocks_3_main_8_bias: \"f32[128]\", p_blocks_3_downsample_0_weight: \"f32[128, 128, 3, 3]\", p_blocks_3_downsample_1_weight: \"f32[128]\", p_blocks_3_downsample_1_bias: \"f32[128]\", p_blocks_4_main_0_weight: \"f32[256, 128, 1, 1]\", p_blocks_4_main_1_weight: \"f32[256]\", p_blocks_4_main_1_bias: \"f32[256]\", p_blocks_4_main_3_weight: \"f32[256, 256, 3, 3]\", p_blocks_4_main_4_weight: \"f32[256]\", p_blocks_4_main_4_bias: \"f32[256]\", p_blocks_4_main_7_weight: \"f32[256, 256, 1, 1]\", p_blocks_4_main_8_weight: \"f32[256]\", p_blocks_4_main_8_bias: \"f32[256]\", p_blocks_4_downsample_0_weight: \"f32[256, 128, 3, 3]\", p_blocks_4_downsample_1_weight: \"f32[256]\", p_blocks_4_downsample_1_bias: \"f32[256]\", p_final_conv_weight: \"f32[256, 256, 3]\", p_final_conv_bias: \"f32[256]\", p_fc_weight: \"f32[88, 40960]\", p_fc_bias: \"f32[88]\", b_preprocess_1_running_mean: \"f32[32]\", b_preprocess_1_running_var: \"f32[32]\", b_preprocess_1_num_batches_tracked: \"i64[]\", b_blocks_0_main_1_running_mean: \"f32[32]\", b_blocks_0_main_1_running_var: \"f32[32]\", b_blocks_0_main_1_num_batches_tracked: \"i64[]\", b_blocks_0_main_4_running_mean: \"f32[32]\", b_blocks_0_main_4_running_var: \"f32[32]\", b_blocks_0_main_4_num_batches_tracked: \"i64[]\", b_blocks_0_main_8_running_mean: \"f32[32]\", b_blocks_0_main_8_running_var: \"f32[32]\", b_blocks_0_main_8_num_batches_tracked: \"i64[]\", b_blocks_0_downsample_1_running_mean: \"f32[32]\", b_blocks_0_downsample_1_running_var: \"f32[32]\", b_blocks_0_downsample_1_num_batches_tracked: \"i64[]\", b_blocks_1_main_1_running_mean: \"f32[64]\", b_blocks_1_main_1_running_var: \"f32[64]\", b_blocks_1_main_1_num_batches_tracked: \"i64[]\", b_blocks_1_main_4_running_mean: \"f32[64]\", b_blocks_1_main_4_running_var: \"f32[64]\", b_blocks_1_main_4_num_batches_tracked: \"i64[]\", b_blocks_1_main_8_running_mean: \"f32[64]\", b_blocks_1_main_8_running_var: \"f32[64]\", b_blocks_1_main_8_num_batches_tracked: \"i64[]\", b_blocks_1_downsample_1_running_mean: \"f32[64]\", b_blocks_1_downsample_1_running_var: \"f32[64]\", b_blocks_1_downsample_1_num_batches_tracked: \"i64[]\", b_blocks_2_main_1_running_mean: \"f32[128]\", b_blocks_2_main_1_running_var: \"f32[128]\", b_blocks_2_main_1_num_batches_tracked: \"i64[]\", b_blocks_2_main_4_running_mean: \"f32[128]\", b_blocks_2_main_4_running_var: \"f32[128]\", b_blocks_2_main_4_num_batches_tracked: \"i64[]\", b_blocks_2_main_8_running_mean: \"f32[128]\", b_blocks_2_main_8_running_var: \"f32[128]\", b_blocks_2_main_8_num_batches_tracked: \"i64[]\", b_blocks_2_downsample_1_running_mean: \"f32[128]\", b_blocks_2_downsample_1_running_var: \"f32[128]\", b_blocks_2_downsample_1_num_batches_tracked: \"i64[]\", b_blocks_3_main_1_running_mean: \"f32[128]\", b_blocks_3_main_1_running_var: \"f32[128]\", b_blocks_3_main_1_num_batches_tracked: \"i64[]\", b_blocks_3_main_4_running_mean: \"f32[128]\", b_blocks_3_main_4_running_var: \"f32[128]\", b_blocks_3_main_4_num_batches_tracked: \"i64[]\", b_blocks_3_main_8_running_mean: \"f32[128]\", b_blocks_3_main_8_running_var: \"f32[128]\", b_blocks_3_main_8_num_batches_tracked: \"i64[]\", b_blocks_3_downsample_1_running_mean: \"f32[128]\", b_blocks_3_downsample_1_running_var: \"f32[128]\", b_blocks_3_downsample_1_num_batches_tracked: \"i64[]\", b_blocks_4_main_1_running_mean: \"f32[256]\", b_blocks_4_main_1_running_var: \"f32[256]\", b_blocks_4_main_1_num_batches_tracked: \"i64[]\", b_blocks_4_main_4_running_mean: \"f32[256]\", b_blocks_4_main_4_running_var: \"f32[256]\", b_blocks_4_main_4_num_batches_tracked: \"i64[]\", b_blocks_4_main_8_running_mean: \"f32[256]\", b_blocks_4_main_8_running_var: \"f32[256]\", b_blocks_4_main_8_num_batches_tracked: \"i64[]\", b_blocks_4_downsample_1_running_mean: \"f32[256]\", b_blocks_4_downsample_1_running_var: \"f32[256]\", b_blocks_4_downsample_1_num_batches_tracked: \"i64[]\", x: \"f32[s77, 1, 480, 640]\"):\n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d: \"f32[s77, 32, 480, 640]\" = torch.ops.aten.conv2d.default(x, p_preprocess_0_weight, p_preprocess_0_bias, [1, 1], [1, 1]);  x = p_preprocess_0_weight = p_preprocess_0_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d, p_preprocess_1_weight, p_preprocess_1_bias, b_preprocess_1_running_mean, b_preprocess_1_running_var, 0.1, 1e-05);  conv2d = p_preprocess_1_weight = p_preprocess_1_bias = b_preprocess_1_running_mean = b_preprocess_1_running_var = None\n",
       "                    getitem: \"f32[1, 32, 480, 640]\" = _native_batch_norm_legit_no_training[0];  _native_batch_norm_legit_no_training = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\activation.py:922 in forward, code: return F.leaky_relu(input, self.negative_slope, self.inplace)\n",
       "                    leaky_relu: \"f32[1, 32, 480, 640]\" = torch.ops.aten.leaky_relu.default(getitem);  getitem = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_1: \"f32[1, 32, 480, 640]\" = torch.ops.aten.conv2d.default(leaky_relu, p_blocks_0_main_0_weight);  p_blocks_0_main_0_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_1 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_1, p_blocks_0_main_1_weight, p_blocks_0_main_1_bias, b_blocks_0_main_1_running_mean, b_blocks_0_main_1_running_var, 0.1, 1e-05);  conv2d_1 = p_blocks_0_main_1_weight = p_blocks_0_main_1_bias = b_blocks_0_main_1_running_mean = b_blocks_0_main_1_running_var = None\n",
       "                    getitem_3: \"f32[1, 32, 480, 640]\" = _native_batch_norm_legit_no_training_1[0];  _native_batch_norm_legit_no_training_1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\activation.py:922 in forward, code: return F.leaky_relu(input, self.negative_slope, self.inplace)\n",
       "                    leaky_relu_1: \"f32[1, 32, 480, 640]\" = torch.ops.aten.leaky_relu.default(getitem_3);  getitem_3 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_2: \"f32[1, 32, 240, 320]\" = torch.ops.aten.conv2d.default(leaky_relu_1, p_blocks_0_main_3_weight, None, [2, 2], [1, 1]);  leaky_relu_1 = p_blocks_0_main_3_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_2 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_2, p_blocks_0_main_4_weight, p_blocks_0_main_4_bias, b_blocks_0_main_4_running_mean, b_blocks_0_main_4_running_var, 0.1, 1e-05);  conv2d_2 = p_blocks_0_main_4_weight = p_blocks_0_main_4_bias = b_blocks_0_main_4_running_mean = b_blocks_0_main_4_running_var = None\n",
       "                    getitem_6: \"f32[1, 32, 240, 320]\" = _native_batch_norm_legit_no_training_2[0];  _native_batch_norm_legit_no_training_2 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\activation.py:922 in forward, code: return F.leaky_relu(input, self.negative_slope, self.inplace)\n",
       "                    leaky_relu_2: \"f32[1, 32, 240, 320]\" = torch.ops.aten.leaky_relu.default(getitem_6);  getitem_6 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone: \"f32[1, 32, 240, 320]\" = torch.ops.aten.clone.default(leaky_relu_2);  leaky_relu_2 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_3: \"f32[1, 32, 240, 320]\" = torch.ops.aten.conv2d.default(clone, p_blocks_0_main_7_weight);  clone = p_blocks_0_main_7_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_3 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_3, p_blocks_0_main_8_weight, p_blocks_0_main_8_bias, b_blocks_0_main_8_running_mean, b_blocks_0_main_8_running_var, 0.1, 1e-05);  conv2d_3 = p_blocks_0_main_8_weight = p_blocks_0_main_8_bias = b_blocks_0_main_8_running_mean = b_blocks_0_main_8_running_var = None\n",
       "                    getitem_9: \"f32[1, 32, 240, 320]\" = _native_batch_norm_legit_no_training_3[0];  _native_batch_norm_legit_no_training_3 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_4: \"f32[1, 32, 240, 320]\" = torch.ops.aten.conv2d.default(leaky_relu, p_blocks_0_downsample_0_weight, None, [2, 2], [1, 1]);  leaky_relu = p_blocks_0_downsample_0_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_4 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_4, p_blocks_0_downsample_1_weight, p_blocks_0_downsample_1_bias, b_blocks_0_downsample_1_running_mean, b_blocks_0_downsample_1_running_var, 0.1, 1e-05);  conv2d_4 = p_blocks_0_downsample_1_weight = p_blocks_0_downsample_1_bias = b_blocks_0_downsample_1_running_mean = b_blocks_0_downsample_1_running_var = None\n",
       "                    getitem_12: \"f32[1, 32, 240, 320]\" = _native_batch_norm_legit_no_training_4[0];  _native_batch_norm_legit_no_training_4 = None\n",
       "            \n",
       "                     # File: C:\\Users\\unova\\AppData\\Local\\Temp\\ipykernel_19328\\2493793081.py:36 in forward, code: return self.relu(self.main(x) + self.downsample(x))\n",
       "                    add_5: \"f32[1, 32, 240, 320]\" = torch.ops.aten.add.Tensor(getitem_9, getitem_12);  getitem_9 = getitem_12 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\activation.py:922 in forward, code: return F.leaky_relu(input, self.negative_slope, self.inplace)\n",
       "                    leaky_relu_3: \"f32[1, 32, 240, 320]\" = torch.ops.aten.leaky_relu.default(add_5);  add_5 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_5: \"f32[1, 64, 240, 320]\" = torch.ops.aten.conv2d.default(leaky_relu_3, p_blocks_1_main_0_weight);  p_blocks_1_main_0_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_5 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_5, p_blocks_1_main_1_weight, p_blocks_1_main_1_bias, b_blocks_1_main_1_running_mean, b_blocks_1_main_1_running_var, 0.1, 1e-05);  conv2d_5 = p_blocks_1_main_1_weight = p_blocks_1_main_1_bias = b_blocks_1_main_1_running_mean = b_blocks_1_main_1_running_var = None\n",
       "                    getitem_15: \"f32[1, 64, 240, 320]\" = _native_batch_norm_legit_no_training_5[0];  _native_batch_norm_legit_no_training_5 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\activation.py:922 in forward, code: return F.leaky_relu(input, self.negative_slope, self.inplace)\n",
       "                    leaky_relu_4: \"f32[1, 64, 240, 320]\" = torch.ops.aten.leaky_relu.default(getitem_15);  getitem_15 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_6: \"f32[1, 64, 120, 160]\" = torch.ops.aten.conv2d.default(leaky_relu_4, p_blocks_1_main_3_weight, None, [2, 2], [1, 1]);  leaky_relu_4 = p_blocks_1_main_3_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_6 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_6, p_blocks_1_main_4_weight, p_blocks_1_main_4_bias, b_blocks_1_main_4_running_mean, b_blocks_1_main_4_running_var, 0.1, 1e-05);  conv2d_6 = p_blocks_1_main_4_weight = p_blocks_1_main_4_bias = b_blocks_1_main_4_running_mean = b_blocks_1_main_4_running_var = None\n",
       "                    getitem_18: \"f32[1, 64, 120, 160]\" = _native_batch_norm_legit_no_training_6[0];  _native_batch_norm_legit_no_training_6 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\activation.py:922 in forward, code: return F.leaky_relu(input, self.negative_slope, self.inplace)\n",
       "                    leaky_relu_5: \"f32[1, 64, 120, 160]\" = torch.ops.aten.leaky_relu.default(getitem_18);  getitem_18 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_1: \"f32[1, 64, 120, 160]\" = torch.ops.aten.clone.default(leaky_relu_5);  leaky_relu_5 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_7: \"f32[1, 64, 120, 160]\" = torch.ops.aten.conv2d.default(clone_1, p_blocks_1_main_7_weight);  clone_1 = p_blocks_1_main_7_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_7 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_7, p_blocks_1_main_8_weight, p_blocks_1_main_8_bias, b_blocks_1_main_8_running_mean, b_blocks_1_main_8_running_var, 0.1, 1e-05);  conv2d_7 = p_blocks_1_main_8_weight = p_blocks_1_main_8_bias = b_blocks_1_main_8_running_mean = b_blocks_1_main_8_running_var = None\n",
       "                    getitem_21: \"f32[1, 64, 120, 160]\" = _native_batch_norm_legit_no_training_7[0];  _native_batch_norm_legit_no_training_7 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_8: \"f32[1, 64, 120, 160]\" = torch.ops.aten.conv2d.default(leaky_relu_3, p_blocks_1_downsample_0_weight, None, [2, 2], [1, 1]);  leaky_relu_3 = p_blocks_1_downsample_0_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_8 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_8, p_blocks_1_downsample_1_weight, p_blocks_1_downsample_1_bias, b_blocks_1_downsample_1_running_mean, b_blocks_1_downsample_1_running_var, 0.1, 1e-05);  conv2d_8 = p_blocks_1_downsample_1_weight = p_blocks_1_downsample_1_bias = b_blocks_1_downsample_1_running_mean = b_blocks_1_downsample_1_running_var = None\n",
       "                    getitem_24: \"f32[1, 64, 120, 160]\" = _native_batch_norm_legit_no_training_8[0];  _native_batch_norm_legit_no_training_8 = None\n",
       "            \n",
       "                     # File: C:\\Users\\unova\\AppData\\Local\\Temp\\ipykernel_19328\\2493793081.py:36 in forward, code: return self.relu(self.main(x) + self.downsample(x))\n",
       "                    add_6: \"f32[1, 64, 120, 160]\" = torch.ops.aten.add.Tensor(getitem_21, getitem_24);  getitem_21 = getitem_24 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\activation.py:922 in forward, code: return F.leaky_relu(input, self.negative_slope, self.inplace)\n",
       "                    leaky_relu_6: \"f32[1, 64, 120, 160]\" = torch.ops.aten.leaky_relu.default(add_6);  add_6 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_9: \"f32[1, 128, 120, 160]\" = torch.ops.aten.conv2d.default(leaky_relu_6, p_blocks_2_main_0_weight);  p_blocks_2_main_0_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_9 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_9, p_blocks_2_main_1_weight, p_blocks_2_main_1_bias, b_blocks_2_main_1_running_mean, b_blocks_2_main_1_running_var, 0.1, 1e-05);  conv2d_9 = p_blocks_2_main_1_weight = p_blocks_2_main_1_bias = b_blocks_2_main_1_running_mean = b_blocks_2_main_1_running_var = None\n",
       "                    getitem_27: \"f32[1, 128, 120, 160]\" = _native_batch_norm_legit_no_training_9[0];  _native_batch_norm_legit_no_training_9 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\activation.py:922 in forward, code: return F.leaky_relu(input, self.negative_slope, self.inplace)\n",
       "                    leaky_relu_7: \"f32[1, 128, 120, 160]\" = torch.ops.aten.leaky_relu.default(getitem_27);  getitem_27 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_10: \"f32[1, 128, 60, 160]\" = torch.ops.aten.conv2d.default(leaky_relu_7, p_blocks_2_main_3_weight, None, [2, 1], [1, 1]);  leaky_relu_7 = p_blocks_2_main_3_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_10 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_10, p_blocks_2_main_4_weight, p_blocks_2_main_4_bias, b_blocks_2_main_4_running_mean, b_blocks_2_main_4_running_var, 0.1, 1e-05);  conv2d_10 = p_blocks_2_main_4_weight = p_blocks_2_main_4_bias = b_blocks_2_main_4_running_mean = b_blocks_2_main_4_running_var = None\n",
       "                    getitem_30: \"f32[1, 128, 60, 160]\" = _native_batch_norm_legit_no_training_10[0];  _native_batch_norm_legit_no_training_10 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\activation.py:922 in forward, code: return F.leaky_relu(input, self.negative_slope, self.inplace)\n",
       "                    leaky_relu_8: \"f32[1, 128, 60, 160]\" = torch.ops.aten.leaky_relu.default(getitem_30);  getitem_30 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_2: \"f32[1, 128, 60, 160]\" = torch.ops.aten.clone.default(leaky_relu_8);  leaky_relu_8 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_11: \"f32[1, 128, 60, 160]\" = torch.ops.aten.conv2d.default(clone_2, p_blocks_2_main_7_weight);  clone_2 = p_blocks_2_main_7_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_11 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_11, p_blocks_2_main_8_weight, p_blocks_2_main_8_bias, b_blocks_2_main_8_running_mean, b_blocks_2_main_8_running_var, 0.1, 1e-05);  conv2d_11 = p_blocks_2_main_8_weight = p_blocks_2_main_8_bias = b_blocks_2_main_8_running_mean = b_blocks_2_main_8_running_var = None\n",
       "                    getitem_33: \"f32[1, 128, 60, 160]\" = _native_batch_norm_legit_no_training_11[0];  _native_batch_norm_legit_no_training_11 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_12: \"f32[1, 128, 60, 160]\" = torch.ops.aten.conv2d.default(leaky_relu_6, p_blocks_2_downsample_0_weight, None, [2, 1], [1, 1]);  leaky_relu_6 = p_blocks_2_downsample_0_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_12 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_12, p_blocks_2_downsample_1_weight, p_blocks_2_downsample_1_bias, b_blocks_2_downsample_1_running_mean, b_blocks_2_downsample_1_running_var, 0.1, 1e-05);  conv2d_12 = p_blocks_2_downsample_1_weight = p_blocks_2_downsample_1_bias = b_blocks_2_downsample_1_running_mean = b_blocks_2_downsample_1_running_var = None\n",
       "                    getitem_36: \"f32[1, 128, 60, 160]\" = _native_batch_norm_legit_no_training_12[0];  _native_batch_norm_legit_no_training_12 = None\n",
       "            \n",
       "                     # File: C:\\Users\\unova\\AppData\\Local\\Temp\\ipykernel_19328\\2493793081.py:36 in forward, code: return self.relu(self.main(x) + self.downsample(x))\n",
       "                    add_7: \"f32[1, 128, 60, 160]\" = torch.ops.aten.add.Tensor(getitem_33, getitem_36);  getitem_33 = getitem_36 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\activation.py:922 in forward, code: return F.leaky_relu(input, self.negative_slope, self.inplace)\n",
       "                    leaky_relu_9: \"f32[1, 128, 60, 160]\" = torch.ops.aten.leaky_relu.default(add_7);  add_7 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_13: \"f32[1, 128, 60, 160]\" = torch.ops.aten.conv2d.default(leaky_relu_9, p_blocks_3_main_0_weight);  p_blocks_3_main_0_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_13 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_13, p_blocks_3_main_1_weight, p_blocks_3_main_1_bias, b_blocks_3_main_1_running_mean, b_blocks_3_main_1_running_var, 0.1, 1e-05);  conv2d_13 = p_blocks_3_main_1_weight = p_blocks_3_main_1_bias = b_blocks_3_main_1_running_mean = b_blocks_3_main_1_running_var = None\n",
       "                    getitem_39: \"f32[1, 128, 60, 160]\" = _native_batch_norm_legit_no_training_13[0];  _native_batch_norm_legit_no_training_13 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\activation.py:922 in forward, code: return F.leaky_relu(input, self.negative_slope, self.inplace)\n",
       "                    leaky_relu_10: \"f32[1, 128, 60, 160]\" = torch.ops.aten.leaky_relu.default(getitem_39);  getitem_39 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_14: \"f32[1, 128, 30, 160]\" = torch.ops.aten.conv2d.default(leaky_relu_10, p_blocks_3_main_3_weight, None, [2, 1], [1, 1]);  leaky_relu_10 = p_blocks_3_main_3_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_14 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_14, p_blocks_3_main_4_weight, p_blocks_3_main_4_bias, b_blocks_3_main_4_running_mean, b_blocks_3_main_4_running_var, 0.1, 1e-05);  conv2d_14 = p_blocks_3_main_4_weight = p_blocks_3_main_4_bias = b_blocks_3_main_4_running_mean = b_blocks_3_main_4_running_var = None\n",
       "                    getitem_42: \"f32[1, 128, 30, 160]\" = _native_batch_norm_legit_no_training_14[0];  _native_batch_norm_legit_no_training_14 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\activation.py:922 in forward, code: return F.leaky_relu(input, self.negative_slope, self.inplace)\n",
       "                    leaky_relu_11: \"f32[1, 128, 30, 160]\" = torch.ops.aten.leaky_relu.default(getitem_42);  getitem_42 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_3: \"f32[1, 128, 30, 160]\" = torch.ops.aten.clone.default(leaky_relu_11);  leaky_relu_11 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_15: \"f32[1, 128, 30, 160]\" = torch.ops.aten.conv2d.default(clone_3, p_blocks_3_main_7_weight);  clone_3 = p_blocks_3_main_7_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_15 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_15, p_blocks_3_main_8_weight, p_blocks_3_main_8_bias, b_blocks_3_main_8_running_mean, b_blocks_3_main_8_running_var, 0.1, 1e-05);  conv2d_15 = p_blocks_3_main_8_weight = p_blocks_3_main_8_bias = b_blocks_3_main_8_running_mean = b_blocks_3_main_8_running_var = None\n",
       "                    getitem_45: \"f32[1, 128, 30, 160]\" = _native_batch_norm_legit_no_training_15[0];  _native_batch_norm_legit_no_training_15 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_16: \"f32[1, 128, 30, 160]\" = torch.ops.aten.conv2d.default(leaky_relu_9, p_blocks_3_downsample_0_weight, None, [2, 1], [1, 1]);  leaky_relu_9 = p_blocks_3_downsample_0_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_16 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_16, p_blocks_3_downsample_1_weight, p_blocks_3_downsample_1_bias, b_blocks_3_downsample_1_running_mean, b_blocks_3_downsample_1_running_var, 0.1, 1e-05);  conv2d_16 = p_blocks_3_downsample_1_weight = p_blocks_3_downsample_1_bias = b_blocks_3_downsample_1_running_mean = b_blocks_3_downsample_1_running_var = None\n",
       "                    getitem_48: \"f32[1, 128, 30, 160]\" = _native_batch_norm_legit_no_training_16[0];  _native_batch_norm_legit_no_training_16 = None\n",
       "            \n",
       "                     # File: C:\\Users\\unova\\AppData\\Local\\Temp\\ipykernel_19328\\2493793081.py:36 in forward, code: return self.relu(self.main(x) + self.downsample(x))\n",
       "                    add_8: \"f32[1, 128, 30, 160]\" = torch.ops.aten.add.Tensor(getitem_45, getitem_48);  getitem_45 = getitem_48 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\activation.py:922 in forward, code: return F.leaky_relu(input, self.negative_slope, self.inplace)\n",
       "                    leaky_relu_12: \"f32[1, 128, 30, 160]\" = torch.ops.aten.leaky_relu.default(add_8);  add_8 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_17: \"f32[1, 256, 30, 160]\" = torch.ops.aten.conv2d.default(leaky_relu_12, p_blocks_4_main_0_weight);  p_blocks_4_main_0_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_17 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_17, p_blocks_4_main_1_weight, p_blocks_4_main_1_bias, b_blocks_4_main_1_running_mean, b_blocks_4_main_1_running_var, 0.1, 1e-05);  conv2d_17 = p_blocks_4_main_1_weight = p_blocks_4_main_1_bias = b_blocks_4_main_1_running_mean = b_blocks_4_main_1_running_var = None\n",
       "                    getitem_51: \"f32[1, 256, 30, 160]\" = _native_batch_norm_legit_no_training_17[0];  _native_batch_norm_legit_no_training_17 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\activation.py:922 in forward, code: return F.leaky_relu(input, self.negative_slope, self.inplace)\n",
       "                    leaky_relu_13: \"f32[1, 256, 30, 160]\" = torch.ops.aten.leaky_relu.default(getitem_51);  getitem_51 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_18: \"f32[1, 256, 15, 160]\" = torch.ops.aten.conv2d.default(leaky_relu_13, p_blocks_4_main_3_weight, None, [2, 1], [1, 1]);  leaky_relu_13 = p_blocks_4_main_3_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_18 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_18, p_blocks_4_main_4_weight, p_blocks_4_main_4_bias, b_blocks_4_main_4_running_mean, b_blocks_4_main_4_running_var, 0.1, 1e-05);  conv2d_18 = p_blocks_4_main_4_weight = p_blocks_4_main_4_bias = b_blocks_4_main_4_running_mean = b_blocks_4_main_4_running_var = None\n",
       "                    getitem_54: \"f32[1, 256, 15, 160]\" = _native_batch_norm_legit_no_training_18[0];  _native_batch_norm_legit_no_training_18 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\activation.py:922 in forward, code: return F.leaky_relu(input, self.negative_slope, self.inplace)\n",
       "                    leaky_relu_14: \"f32[1, 256, 15, 160]\" = torch.ops.aten.leaky_relu.default(getitem_54);  getitem_54 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_4: \"f32[1, 256, 15, 160]\" = torch.ops.aten.clone.default(leaky_relu_14);  leaky_relu_14 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_19: \"f32[1, 256, 15, 160]\" = torch.ops.aten.conv2d.default(clone_4, p_blocks_4_main_7_weight);  clone_4 = p_blocks_4_main_7_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_19 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_19, p_blocks_4_main_8_weight, p_blocks_4_main_8_bias, b_blocks_4_main_8_running_mean, b_blocks_4_main_8_running_var, 0.1, 1e-05);  conv2d_19 = p_blocks_4_main_8_weight = p_blocks_4_main_8_bias = b_blocks_4_main_8_running_mean = b_blocks_4_main_8_running_var = None\n",
       "                    getitem_57: \"f32[1, 256, 15, 160]\" = _native_batch_norm_legit_no_training_19[0];  _native_batch_norm_legit_no_training_19 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_20: \"f32[1, 256, 15, 160]\" = torch.ops.aten.conv2d.default(leaky_relu_12, p_blocks_4_downsample_0_weight, None, [2, 1], [1, 1]);  leaky_relu_12 = p_blocks_4_downsample_0_weight = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_20 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_20, p_blocks_4_downsample_1_weight, p_blocks_4_downsample_1_bias, b_blocks_4_downsample_1_running_mean, b_blocks_4_downsample_1_running_var, 0.1, 1e-05);  conv2d_20 = p_blocks_4_downsample_1_weight = p_blocks_4_downsample_1_bias = b_blocks_4_downsample_1_running_mean = b_blocks_4_downsample_1_running_var = None\n",
       "                    getitem_60: \"f32[1, 256, 15, 160]\" = _native_batch_norm_legit_no_training_20[0];  _native_batch_norm_legit_no_training_20 = None\n",
       "            \n",
       "                     # File: C:\\Users\\unova\\AppData\\Local\\Temp\\ipykernel_19328\\2493793081.py:36 in forward, code: return self.relu(self.main(x) + self.downsample(x))\n",
       "                    add_9: \"f32[1, 256, 15, 160]\" = torch.ops.aten.add.Tensor(getitem_57, getitem_60);  getitem_57 = getitem_60 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\activation.py:922 in forward, code: return F.leaky_relu(input, self.negative_slope, self.inplace)\n",
       "                    leaky_relu_15: \"f32[1, 256, 15, 160]\" = torch.ops.aten.leaky_relu.default(add_9);  add_9 = None\n",
       "            \n",
       "                     # File: C:\\Users\\unova\\AppData\\Local\\Temp\\ipykernel_19328\\2493793081.py:62 in forward, code: x = x.mean(dim=2)\n",
       "                    mean: \"f32[1, 256, 160]\" = torch.ops.aten.mean.dim(leaky_relu_15, [2]);  leaky_relu_15 = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv1d: \"f32[1, 256, 160]\" = torch.ops.aten.conv1d.default(mean, p_final_conv_weight, p_final_conv_bias, [1], [1]);  mean = p_final_conv_weight = p_final_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\unova\\AppData\\Local\\Temp\\ipykernel_19328\\2493793081.py:64 in forward, code: x = x.flatten(1)\n",
       "                    view: \"f32[1, 40960]\" = torch.ops.aten.view.default(conv1d, [1, 40960]);  conv1d = None\n",
       "            \n",
       "                     # File: c:\\Users\\unova\\anaconda3\\envs\\piano_app\\lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear: \"f32[1, 88]\" = torch.ops.aten.linear.default(view, p_fc_weight, p_fc_bias);  view = p_fc_weight = p_fc_bias = None\n",
       "                    return (linear,)\n",
       "            \n",
       "        Graph signature: \n",
       "            # inputs\n",
       "            p_preprocess_0_weight: PARAMETER target='preprocess.0.weight'\n",
       "            p_preprocess_0_bias: PARAMETER target='preprocess.0.bias'\n",
       "            p_preprocess_1_weight: PARAMETER target='preprocess.1.weight'\n",
       "            p_preprocess_1_bias: PARAMETER target='preprocess.1.bias'\n",
       "            p_blocks_0_main_0_weight: PARAMETER target='blocks.0.main.0.weight'\n",
       "            p_blocks_0_main_1_weight: PARAMETER target='blocks.0.main.1.weight'\n",
       "            p_blocks_0_main_1_bias: PARAMETER target='blocks.0.main.1.bias'\n",
       "            p_blocks_0_main_3_weight: PARAMETER target='blocks.0.main.3.weight'\n",
       "            p_blocks_0_main_4_weight: PARAMETER target='blocks.0.main.4.weight'\n",
       "            p_blocks_0_main_4_bias: PARAMETER target='blocks.0.main.4.bias'\n",
       "            p_blocks_0_main_7_weight: PARAMETER target='blocks.0.main.7.weight'\n",
       "            p_blocks_0_main_8_weight: PARAMETER target='blocks.0.main.8.weight'\n",
       "            p_blocks_0_main_8_bias: PARAMETER target='blocks.0.main.8.bias'\n",
       "            p_blocks_0_downsample_0_weight: PARAMETER target='blocks.0.downsample.0.weight'\n",
       "            p_blocks_0_downsample_1_weight: PARAMETER target='blocks.0.downsample.1.weight'\n",
       "            p_blocks_0_downsample_1_bias: PARAMETER target='blocks.0.downsample.1.bias'\n",
       "            p_blocks_1_main_0_weight: PARAMETER target='blocks.1.main.0.weight'\n",
       "            p_blocks_1_main_1_weight: PARAMETER target='blocks.1.main.1.weight'\n",
       "            p_blocks_1_main_1_bias: PARAMETER target='blocks.1.main.1.bias'\n",
       "            p_blocks_1_main_3_weight: PARAMETER target='blocks.1.main.3.weight'\n",
       "            p_blocks_1_main_4_weight: PARAMETER target='blocks.1.main.4.weight'\n",
       "            p_blocks_1_main_4_bias: PARAMETER target='blocks.1.main.4.bias'\n",
       "            p_blocks_1_main_7_weight: PARAMETER target='blocks.1.main.7.weight'\n",
       "            p_blocks_1_main_8_weight: PARAMETER target='blocks.1.main.8.weight'\n",
       "            p_blocks_1_main_8_bias: PARAMETER target='blocks.1.main.8.bias'\n",
       "            p_blocks_1_downsample_0_weight: PARAMETER target='blocks.1.downsample.0.weight'\n",
       "            p_blocks_1_downsample_1_weight: PARAMETER target='blocks.1.downsample.1.weight'\n",
       "            p_blocks_1_downsample_1_bias: PARAMETER target='blocks.1.downsample.1.bias'\n",
       "            p_blocks_2_main_0_weight: PARAMETER target='blocks.2.main.0.weight'\n",
       "            p_blocks_2_main_1_weight: PARAMETER target='blocks.2.main.1.weight'\n",
       "            p_blocks_2_main_1_bias: PARAMETER target='blocks.2.main.1.bias'\n",
       "            p_blocks_2_main_3_weight: PARAMETER target='blocks.2.main.3.weight'\n",
       "            p_blocks_2_main_4_weight: PARAMETER target='blocks.2.main.4.weight'\n",
       "            p_blocks_2_main_4_bias: PARAMETER target='blocks.2.main.4.bias'\n",
       "            p_blocks_2_main_7_weight: PARAMETER target='blocks.2.main.7.weight'\n",
       "            p_blocks_2_main_8_weight: PARAMETER target='blocks.2.main.8.weight'\n",
       "            p_blocks_2_main_8_bias: PARAMETER target='blocks.2.main.8.bias'\n",
       "            p_blocks_2_downsample_0_weight: PARAMETER target='blocks.2.downsample.0.weight'\n",
       "            p_blocks_2_downsample_1_weight: PARAMETER target='blocks.2.downsample.1.weight'\n",
       "            p_blocks_2_downsample_1_bias: PARAMETER target='blocks.2.downsample.1.bias'\n",
       "            p_blocks_3_main_0_weight: PARAMETER target='blocks.3.main.0.weight'\n",
       "            p_blocks_3_main_1_weight: PARAMETER target='blocks.3.main.1.weight'\n",
       "            p_blocks_3_main_1_bias: PARAMETER target='blocks.3.main.1.bias'\n",
       "            p_blocks_3_main_3_weight: PARAMETER target='blocks.3.main.3.weight'\n",
       "            p_blocks_3_main_4_weight: PARAMETER target='blocks.3.main.4.weight'\n",
       "            p_blocks_3_main_4_bias: PARAMETER target='blocks.3.main.4.bias'\n",
       "            p_blocks_3_main_7_weight: PARAMETER target='blocks.3.main.7.weight'\n",
       "            p_blocks_3_main_8_weight: PARAMETER target='blocks.3.main.8.weight'\n",
       "            p_blocks_3_main_8_bias: PARAMETER target='blocks.3.main.8.bias'\n",
       "            p_blocks_3_downsample_0_weight: PARAMETER target='blocks.3.downsample.0.weight'\n",
       "            p_blocks_3_downsample_1_weight: PARAMETER target='blocks.3.downsample.1.weight'\n",
       "            p_blocks_3_downsample_1_bias: PARAMETER target='blocks.3.downsample.1.bias'\n",
       "            p_blocks_4_main_0_weight: PARAMETER target='blocks.4.main.0.weight'\n",
       "            p_blocks_4_main_1_weight: PARAMETER target='blocks.4.main.1.weight'\n",
       "            p_blocks_4_main_1_bias: PARAMETER target='blocks.4.main.1.bias'\n",
       "            p_blocks_4_main_3_weight: PARAMETER target='blocks.4.main.3.weight'\n",
       "            p_blocks_4_main_4_weight: PARAMETER target='blocks.4.main.4.weight'\n",
       "            p_blocks_4_main_4_bias: PARAMETER target='blocks.4.main.4.bias'\n",
       "            p_blocks_4_main_7_weight: PARAMETER target='blocks.4.main.7.weight'\n",
       "            p_blocks_4_main_8_weight: PARAMETER target='blocks.4.main.8.weight'\n",
       "            p_blocks_4_main_8_bias: PARAMETER target='blocks.4.main.8.bias'\n",
       "            p_blocks_4_downsample_0_weight: PARAMETER target='blocks.4.downsample.0.weight'\n",
       "            p_blocks_4_downsample_1_weight: PARAMETER target='blocks.4.downsample.1.weight'\n",
       "            p_blocks_4_downsample_1_bias: PARAMETER target='blocks.4.downsample.1.bias'\n",
       "            p_final_conv_weight: PARAMETER target='final_conv.weight'\n",
       "            p_final_conv_bias: PARAMETER target='final_conv.bias'\n",
       "            p_fc_weight: PARAMETER target='fc.weight'\n",
       "            p_fc_bias: PARAMETER target='fc.bias'\n",
       "            b_preprocess_1_running_mean: BUFFER target='preprocess.1.running_mean' persistent=True\n",
       "            b_preprocess_1_running_var: BUFFER target='preprocess.1.running_var' persistent=True\n",
       "            b_preprocess_1_num_batches_tracked: BUFFER target='preprocess.1.num_batches_tracked' persistent=True\n",
       "            b_blocks_0_main_1_running_mean: BUFFER target='blocks.0.main.1.running_mean' persistent=True\n",
       "            b_blocks_0_main_1_running_var: BUFFER target='blocks.0.main.1.running_var' persistent=True\n",
       "            b_blocks_0_main_1_num_batches_tracked: BUFFER target='blocks.0.main.1.num_batches_tracked' persistent=True\n",
       "            b_blocks_0_main_4_running_mean: BUFFER target='blocks.0.main.4.running_mean' persistent=True\n",
       "            b_blocks_0_main_4_running_var: BUFFER target='blocks.0.main.4.running_var' persistent=True\n",
       "            b_blocks_0_main_4_num_batches_tracked: BUFFER target='blocks.0.main.4.num_batches_tracked' persistent=True\n",
       "            b_blocks_0_main_8_running_mean: BUFFER target='blocks.0.main.8.running_mean' persistent=True\n",
       "            b_blocks_0_main_8_running_var: BUFFER target='blocks.0.main.8.running_var' persistent=True\n",
       "            b_blocks_0_main_8_num_batches_tracked: BUFFER target='blocks.0.main.8.num_batches_tracked' persistent=True\n",
       "            b_blocks_0_downsample_1_running_mean: BUFFER target='blocks.0.downsample.1.running_mean' persistent=True\n",
       "            b_blocks_0_downsample_1_running_var: BUFFER target='blocks.0.downsample.1.running_var' persistent=True\n",
       "            b_blocks_0_downsample_1_num_batches_tracked: BUFFER target='blocks.0.downsample.1.num_batches_tracked' persistent=True\n",
       "            b_blocks_1_main_1_running_mean: BUFFER target='blocks.1.main.1.running_mean' persistent=True\n",
       "            b_blocks_1_main_1_running_var: BUFFER target='blocks.1.main.1.running_var' persistent=True\n",
       "            b_blocks_1_main_1_num_batches_tracked: BUFFER target='blocks.1.main.1.num_batches_tracked' persistent=True\n",
       "            b_blocks_1_main_4_running_mean: BUFFER target='blocks.1.main.4.running_mean' persistent=True\n",
       "            b_blocks_1_main_4_running_var: BUFFER target='blocks.1.main.4.running_var' persistent=True\n",
       "            b_blocks_1_main_4_num_batches_tracked: BUFFER target='blocks.1.main.4.num_batches_tracked' persistent=True\n",
       "            b_blocks_1_main_8_running_mean: BUFFER target='blocks.1.main.8.running_mean' persistent=True\n",
       "            b_blocks_1_main_8_running_var: BUFFER target='blocks.1.main.8.running_var' persistent=True\n",
       "            b_blocks_1_main_8_num_batches_tracked: BUFFER target='blocks.1.main.8.num_batches_tracked' persistent=True\n",
       "            b_blocks_1_downsample_1_running_mean: BUFFER target='blocks.1.downsample.1.running_mean' persistent=True\n",
       "            b_blocks_1_downsample_1_running_var: BUFFER target='blocks.1.downsample.1.running_var' persistent=True\n",
       "            b_blocks_1_downsample_1_num_batches_tracked: BUFFER target='blocks.1.downsample.1.num_batches_tracked' persistent=True\n",
       "            b_blocks_2_main_1_running_mean: BUFFER target='blocks.2.main.1.running_mean' persistent=True\n",
       "            b_blocks_2_main_1_running_var: BUFFER target='blocks.2.main.1.running_var' persistent=True\n",
       "            b_blocks_2_main_1_num_batches_tracked: BUFFER target='blocks.2.main.1.num_batches_tracked' persistent=True\n",
       "            b_blocks_2_main_4_running_mean: BUFFER target='blocks.2.main.4.running_mean' persistent=True\n",
       "            b_blocks_2_main_4_running_var: BUFFER target='blocks.2.main.4.running_var' persistent=True\n",
       "            b_blocks_2_main_4_num_batches_tracked: BUFFER target='blocks.2.main.4.num_batches_tracked' persistent=True\n",
       "            b_blocks_2_main_8_running_mean: BUFFER target='blocks.2.main.8.running_mean' persistent=True\n",
       "            b_blocks_2_main_8_running_var: BUFFER target='blocks.2.main.8.running_var' persistent=True\n",
       "            b_blocks_2_main_8_num_batches_tracked: BUFFER target='blocks.2.main.8.num_batches_tracked' persistent=True\n",
       "            b_blocks_2_downsample_1_running_mean: BUFFER target='blocks.2.downsample.1.running_mean' persistent=True\n",
       "            b_blocks_2_downsample_1_running_var: BUFFER target='blocks.2.downsample.1.running_var' persistent=True\n",
       "            b_blocks_2_downsample_1_num_batches_tracked: BUFFER target='blocks.2.downsample.1.num_batches_tracked' persistent=True\n",
       "            b_blocks_3_main_1_running_mean: BUFFER target='blocks.3.main.1.running_mean' persistent=True\n",
       "            b_blocks_3_main_1_running_var: BUFFER target='blocks.3.main.1.running_var' persistent=True\n",
       "            b_blocks_3_main_1_num_batches_tracked: BUFFER target='blocks.3.main.1.num_batches_tracked' persistent=True\n",
       "            b_blocks_3_main_4_running_mean: BUFFER target='blocks.3.main.4.running_mean' persistent=True\n",
       "            b_blocks_3_main_4_running_var: BUFFER target='blocks.3.main.4.running_var' persistent=True\n",
       "            b_blocks_3_main_4_num_batches_tracked: BUFFER target='blocks.3.main.4.num_batches_tracked' persistent=True\n",
       "            b_blocks_3_main_8_running_mean: BUFFER target='blocks.3.main.8.running_mean' persistent=True\n",
       "            b_blocks_3_main_8_running_var: BUFFER target='blocks.3.main.8.running_var' persistent=True\n",
       "            b_blocks_3_main_8_num_batches_tracked: BUFFER target='blocks.3.main.8.num_batches_tracked' persistent=True\n",
       "            b_blocks_3_downsample_1_running_mean: BUFFER target='blocks.3.downsample.1.running_mean' persistent=True\n",
       "            b_blocks_3_downsample_1_running_var: BUFFER target='blocks.3.downsample.1.running_var' persistent=True\n",
       "            b_blocks_3_downsample_1_num_batches_tracked: BUFFER target='blocks.3.downsample.1.num_batches_tracked' persistent=True\n",
       "            b_blocks_4_main_1_running_mean: BUFFER target='blocks.4.main.1.running_mean' persistent=True\n",
       "            b_blocks_4_main_1_running_var: BUFFER target='blocks.4.main.1.running_var' persistent=True\n",
       "            b_blocks_4_main_1_num_batches_tracked: BUFFER target='blocks.4.main.1.num_batches_tracked' persistent=True\n",
       "            b_blocks_4_main_4_running_mean: BUFFER target='blocks.4.main.4.running_mean' persistent=True\n",
       "            b_blocks_4_main_4_running_var: BUFFER target='blocks.4.main.4.running_var' persistent=True\n",
       "            b_blocks_4_main_4_num_batches_tracked: BUFFER target='blocks.4.main.4.num_batches_tracked' persistent=True\n",
       "            b_blocks_4_main_8_running_mean: BUFFER target='blocks.4.main.8.running_mean' persistent=True\n",
       "            b_blocks_4_main_8_running_var: BUFFER target='blocks.4.main.8.running_var' persistent=True\n",
       "            b_blocks_4_main_8_num_batches_tracked: BUFFER target='blocks.4.main.8.num_batches_tracked' persistent=True\n",
       "            b_blocks_4_downsample_1_running_mean: BUFFER target='blocks.4.downsample.1.running_mean' persistent=True\n",
       "            b_blocks_4_downsample_1_running_var: BUFFER target='blocks.4.downsample.1.running_var' persistent=True\n",
       "            b_blocks_4_downsample_1_num_batches_tracked: BUFFER target='blocks.4.downsample.1.num_batches_tracked' persistent=True\n",
       "            x: USER_INPUT\n",
       "    \n",
       "            # outputs\n",
       "            linear: USER_OUTPUT\n",
       "    \n",
       "        Range constraints: {s77: VR[0, int_oo]}\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from inference_sdk import InferenceHTTPClient\n",
    "import supervision as sv\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "class PianoModelBlock2D(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, ksize=(3,3), stride=(1,1), drop=0.0, pad=True):\n",
    "        super().__init__()\n",
    "        padding = (ksize[0]//2, ksize[1]//2) if pad else (0,0)\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, out_dim, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(out_dim, out_dim, kernel_size=ksize, stride=stride, padding=padding, bias=False),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=drop),\n",
    "            nn.Conv2d(out_dim, out_dim, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_dim)\n",
    "        )\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, out_dim, kernel_size=ksize, stride=stride, padding=padding, bias=False),\n",
    "            nn.BatchNorm2d(out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return self.relu(self.main(x) + self.downsample(x))\n",
    "\n",
    "class PianoModelSmall2D(nn.Module):\n",
    "    def __init__(self, input_size=(480,640)):\n",
    "        super().__init__()\n",
    "        downscale_dim_sizes = [32,32,64,128,128,256]\n",
    "        self.preprocess = nn.Sequential(\n",
    "            nn.Conv2d(1, downscale_dim_sizes[0], kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(downscale_dim_sizes[0]),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.blocks = nn.ModuleList([\n",
    "            PianoModelBlock2D(downscale_dim_sizes[0], downscale_dim_sizes[1], stride=2, drop=0.2),\n",
    "            PianoModelBlock2D(downscale_dim_sizes[1], downscale_dim_sizes[2], stride=2, drop=0.2),\n",
    "            PianoModelBlock2D(downscale_dim_sizes[2], downscale_dim_sizes[3], stride=(2,1), drop=0.2),\n",
    "            PianoModelBlock2D(downscale_dim_sizes[3], downscale_dim_sizes[4], stride=(2,1), drop=0.2),\n",
    "            PianoModelBlock2D(downscale_dim_sizes[4], downscale_dim_sizes[5], stride=(2,1), drop=0.0)\n",
    "        ])\n",
    "        final_conv_dim = 256\n",
    "        self.final_conv = nn.Conv1d(downscale_dim_sizes[-1], final_conv_dim, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear((input_size[1]//4)*final_conv_dim, 88)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = self.preprocess(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = x.mean(dim=2)\n",
    "        x = self.final_conv(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "device = \"cpu\"  # use CPU for export\n",
    "model = torch.load(\"model/final_model.pt\", map_location=device, weights_only=False)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "dummy_input = torch.randn(1, 1, 480, 640, device=device)\n",
    "\n",
    "onnx_file_path = \"model/final_model.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_file_path,\n",
    "    export_params=True,           \n",
    "    opset_version=17,             \n",
    "    do_constant_folding=True,     \n",
    "    input_names=['input'],       \n",
    "    output_names=['output'],      \n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'}, \n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68665c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (1, 88)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "  \n",
    "onnx_model = onnx.load(onnx_file_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "sess = ort.InferenceSession(onnx_file_path)\n",
    "\n",
    "test_input = np.random.randn(1,1,480,640).astype(np.float32)\n",
    "outputs = sess.run(None, {\"input\": test_input})\n",
    "print(\"Output shape:\", outputs[0].shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piano_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
